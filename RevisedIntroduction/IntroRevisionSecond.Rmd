---
title: "Intro test" 
author: "Thomas Fiore"
date: "`r format.Date(Sys.Date(), '%B %d, %Y')`"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE, message=FALSE)
```

The focus of this tutorial is *analysis of variance (ANOVA)*. ANOVA is a statistical approach to compare means of an outcome variable of interest across different groups. If there are only two groups, the testing problem could simply use a $t$-test. But ANOVA makes it possible to test a multiple null hypothesis that compares several groups simultaneously, which would otherwise be tested by several $t$-tests with larger type I error.

The assumptions for parametric (fixed-effects) ANOVA are:

1. the errors are normally distributed 
2. the errors have mean 0
3. the errors are independent
4. the errors have equal variance among the groups.

A first diagnostic test is to plot the residuals against the predicted responses and against treatment levels. Normality can be checked with a qq-plot as usual. Fortunately, ANOVA works well when the assumptions are nearly satisfied. 

The **parametric** one-way ANOVA has a simple form for $y_{ij}$, the $j$th observed response of group $i$:
$$
y_{ij} = u + \mu_i+\epsilon_{ij}
$$
where $u$ is the grand mean, each group $i$ has unique group mean $u+\mu_i$, and  $\epsilon_{ij}$ is i.i.d. $N(0,\sigma^2)$. If there is more than one grouping variable, say 2 grouping variables as an example, with the same assumptions, the two-way ANOVA has a slightly more complex form for $y_{ijk}$, the $k$th observed reponse of group $ij$:
$$
y_{ijk} = u + \mu_i + \eta_j + \gamma_{ij} + \epsilon_{ijk}
$$
where $u$ is again the grand mean, the mean of group $ij$ is $\mu_i + \eta_j + \gamma_{ij}$, and $\epsilon_{ijk}$ is i.i.d.  $N(0,\sigma^2)$. The terms in the mean of group $ij$ can be interpreted as follows: $\mu_i$ is the mean effect of the first grouping variable, $\eta_j$ is the mean effect of the second grouping variable, and the interaction term $\gamma_{ij}$ is ``the failure of the response to one factor to be the same at different levels of another factor'' (see [PSU site](https://onlinecourses.science.psu.edu/stat500/node/216/)).   ANOVA analysis is simply attributing the grand variations in the outcome variables into the variation caused by the specific group means and the unexplainable random noises. And then ANOVA compares the variations attributable to specific group means and those attributed to the noises and see if the group means are responsible for a relatively large size of the variation.

To do inference for one-way ANOVA, one uses the $F$-statistic for one-way ANOVA:
$$F=\frac{\text{variation between sample means}}{\text{variation within the samples}}.$$ For more on the $F$-statistic, hypothesis testing, and $p$-values, see this [Minitab Blog entry](http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-analysis-of-variance-anova-and-the-f-test).

Sometimes the assumptions for the parametric ANOVA above are not satisfied, and we could instead turn to a nonparametric counterpart of ANOVA, called *Kruskal-Wallis test*. The Kruskal-Wallis test simply transforms the original outcome variable data into the ranks of the data and then tests whether group mean ranks are different. Based on normality, the parametric ANOVA uses F-test while the Kruskal-Wallis test uses permutation test instead, which typically has more power in non-normal cases. In this tutorial, we would briefly go over one-way ANOVA, two-way ANOVA, and the Kruskal-Wallis test in R, STATA, and MATLAB.

Since the ANOVA could only tell us whether the group means of all groups are different, we still need to identify which groups are actually different by doing multiple comparisons across different group pairs. For ANOVA results, a specific multiple comparison approach could be used, which is called the *Tukey honest significance test*. The test simply compares pairwise group means but just like many other multiple comparison ($p$-value adjustment) methods, it controls for family-wise error rate (FWER).
We will also demonstrate how to do this post-estimation step in all three languages. For a deeper and detailed introduction to one-way ANOVA and Tukey's HSD, please refer to [this](https://www.spss-tutorials.com/anova-what-is-it/#assumptions). For two-way ANOVA, the tutorial at Penn State [here](https://onlinecourses.science.psu.edu/stat500/node/216/) is helpful. And for Kruskal-Wallis test, refer to Section 6.1 in book *Nonparametric Statistical Methods* by Hollander and Wolfe for a more theoretical treatment.

The data set we use for the illustration is the **diet** data, which is a small survey of several male and female adults who take on three different diet plans. And the focus of the survey is people's body weight changes during a six-week time period. A link to the data is [here](https://www.sheffield.ac.uk/mash/statistics2/anova). 
The self-descriptive variables of interest are the response variable weightlost, and the two grouping factors Diet and gender. For ANOVA, we are considering the mean of weightlost because of gender, Diet, and lastly because of the effect of diet according to gender (or vice-a-versa).

In the R, STATA, and MATLAB tutorials below, we first do a one-way ANOVA for weight loss for only females and for only males across the three diets and make a graph that shows the confidence intervals (adjusted for multicomparisons) for the three weight loss group means. Next we do two-way ANOVA for weight loss for males/females across the three diets. Finally, we again do the initial task of one-way ANOVA for weight loss for only females and for only males across the three diets, but use the Kruskal-Wallis test instead of parametric ANOVA.

The most important commands are aov() in R, anova in SAS, and multcompare in MATLAB.  

# References
1. [SPSS Tutorial: ANOVA - Simple Introduction](https://www.spss-tutorials.com/anova-what-is-it)

2. [Pennsylvania State University Online Course Statistics 500: Applied Statistics](https://onlinecourses.science.psu.edu/stat500/node/216/) [see Section 11.2]

3. Myles Hollander and Douglas A. Wolfe. *Nonparametric Statistical Methods*, Wiley Series in Probability and Statistics, 1999. [see Section 6.1]

4. Paul Teetor. *R Cookbook*, O'Reilly Media, 2011. [See Sections 11.20, 11.23]